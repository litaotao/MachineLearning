{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯 概述 \n",
    "\n",
    "`贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种: 朴素贝叶斯分类。`\n",
    "\n",
    "## 贝叶斯理论 & 条件概率\n",
    "\n",
    "### 贝叶斯理论\n",
    "\n",
    "我们现在有一个数据集，它由两类数据组成，数据分布如下图所示：\n",
    "\n",
    "![朴素贝叶斯示例数据分布](../../../images/4.NaiveBayesian/朴素贝叶斯示例数据分布.png \"参数已知的概率分布\")\n",
    "\n",
    "我们现在用 p1(x,y) 表示数据点 (x,y) 属于类别 1（图中用圆点表示的类别）的概率，用 p2(x,y) 表示数据点 (x,y) 属于类别 2（图中三角形表示的类别）的概率，那么对于一个新数据点 (x,y)，可以用下面的规则来判断它的类别：\n",
    "* 如果 p1(x,y) > p2(x,y) ，那么类别为1\n",
    "* 如果 p2(x,y) > p1(x,y) ，那么类别为2\n",
    "\n",
    "也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。\n",
    "\n",
    "### 条件概率\n",
    "\n",
    "如果你对 p(x,y|c1) 符号很熟悉，那么可以跳过本小节。\n",
    "\n",
    "有一个装了 7 块石头的罐子，其中 3 块是白色的，4 块是黑色的。如果从罐子中随机取出一块石头，那么是白色石头的可能性是多少？由于取石头有 7 种可能，其中 3 种为白色，所以取出白色石头的概率为 3/7 。那么取到黑色石头的概率又是多少呢？很显然，是 4/7 。我们使用 P(white) 来表示取到白色石头的概率，其概率值可以通过白色石头数目除以总的石头数目来得到。\n",
    "\n",
    "![包含 7 块石头的集合](../../../images/4.NaiveBayesian/NB_2.png)\n",
    "\n",
    "如果这 7 块石头如下图所示，放在两个桶中，那么上述概率应该如何计算？\n",
    "\n",
    "![7块石头放入两个桶中](../../../images/4.NaiveBayesian/NB_3.png)\n",
    "\n",
    "计算 P(white) 或者 P(black) ，如果事先我们知道石头所在桶的信息是会改变结果的。这就是所谓的条件概率（conditional probablity）。假定计算的是从 B 桶取到白色石头的概率，这个概率可以记作 P(white|bucketB) ，我们称之为“在已知石头出自 B 桶的条件下，取出白色石头的概率”。很容易得到，P(white|bucketA) 值为 2/4 ，P(white|bucketB) 的值为 1/3 。\n",
    "\n",
    "条件概率的计算公式如下：\n",
    "\n",
    "$$ P(white|bucketB) = \\frac {P(white \\ and \\ bucketB)} {P(bucketB)} $$\n",
    "\n",
    "首先，我们用 B 桶中白色石头的个数除以两个桶中总的石头数，得到 P(white and bucketB) = 1/7 .其次，由于 B 桶中有 3 块石头，而总石头数为 7 ，于是 P(bucketB) 就等于 3/7 。于是又 P(white|bucketB) = P(white and bucketB) / P(bucketB) = (1/7) / (3/7) = 1/3 。\n",
    "\n",
    "另外一种有效计算条件概率的方法称为贝叶斯准则。贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知 P(x|c)，要求 P(c|x)，那么可以使用下面的计算方法：\n",
    "\n",
    "![计算p(c|x)的方法](../../../images/4.NaiveBayesian/NB_4.png)\n",
    "\n",
    "### 使用条件概率来分类\n",
    "\n",
    "上面我们提到贝叶斯决策理论要求计算两个概率 p1(x, y) 和 p2(x, y):\n",
    "* 如果 p1(x, y) > p2(x, y), 那么属于类别 1;\n",
    "* 如果 p2(x, y) > p1(X, y), 那么属于类别 2.\n",
    "\n",
    "这并不是贝叶斯决策理论的所有内容。使用 p1() 和 p2() 只是为了尽可能简化描述，而真正需要计算和比较的是 p(c1|x, y) 和 p(c2|x, y) .这些符号所代表的具体意义是: 给定某个由 x、y 表示的数据点，那么该数据点来自类别 c1 的概率是多少？数据点来自类别 c2 的概率又是多少？注意这些概率与概率 p(x, y|c1) 并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到: \n",
    "\n",
    "![应用贝叶斯准则](../../../images/4.NaiveBayesian/NB_5.png)\n",
    "\n",
    "使用上面这些定义，可以定义贝叶斯分类准则为:\n",
    "* 如果 P(c1|x, y) > P(c2|x, y), 那么属于类别 c1;\n",
    "* 如果 P(c2|x, y) > P(c1|x, y), 那么属于类别 c2.\n",
    "\n",
    "在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。\n",
    "\n",
    "我们假设特征之间  **相互独立** 。所谓 <b>独立(independence)</b> 指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，比如说，“我们”中的“我”和“们”出现的概率与这两个字相邻没有任何关系。这个假设正是朴素贝叶斯分类器中 朴素(naive) 一词的含义。朴素贝叶斯分类器中的另一个假设是，<b>每个特征同等重要</b>。\n",
    "\n",
    "<b>Note:</b> 朴素贝叶斯分类器通常有两种实现方式: 一种基于伯努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的。\n",
    "\n",
    "## 朴素贝叶斯 场景\n",
    "\n",
    "机器学习的一个重要应用就是文档的自动分类。\n",
    "\n",
    "在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。\n",
    "\n",
    "朴素贝叶斯是上面介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。下面我们会进行一些朴素贝叶斯分类的实践项目。\n",
    "\n",
    "## 朴素贝叶斯 原理\n",
    "\n",
    "### 朴素贝叶斯 工作原理\n",
    "\n",
    "```\n",
    "提取所有文档中的词条并进行去重\n",
    "获取文档的所有类别\n",
    "计算每个类别中的文档数目\n",
    "对每篇训练文档: \n",
    "    对每个类别: \n",
    "        如果词条出现在文档中-->增加该词条的计数值（for循环或者矩阵相加）\n",
    "        增加所有词条的计数值（此类别下词条总数）\n",
    "对每个类别: \n",
    "    对每个词条: \n",
    "        将该词条的数目除以总词条数目得到的条件概率（P(词条|类别)）\n",
    "返回该文档属于每个类别的条件概率（P(类别|文档的所有词条)）\n",
    "```\n",
    "\n",
    "### 朴素贝叶斯 开发流程\n",
    "\n",
    "```\n",
    "收集数据: 可以使用任何方法。\n",
    "准备数据: 需要数值型或者布尔型数据。\n",
    "分析数据: 有大量特征时，绘制特征作用不大，此时使用直方图效果更好。\n",
    "训练算法: 计算不同的独立特征的条件概率。\n",
    "测试算法: 计算错误率。\n",
    "使用算法: 一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。\n",
    "```\n",
    "\n",
    "### 朴素贝叶斯 算法特点\n",
    "\n",
    "```\n",
    "优点: 在数据较少的情况下仍然有效，可以处理多类别问题。\n",
    "缺点: 对于输入数据的准备方式较为敏感。\n",
    "适用数据类型: 标称型数据。\n",
    "```\n",
    "\n",
    "## 朴素贝叶斯 项目案例\n",
    "\n",
    "### 项目案例1: 屏蔽社区留言板的侮辱性言论\n",
    "\n",
    "#### 项目概述\n",
    "\n",
    "构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此问题建立两个类别: 侮辱类和非侮辱类，使用 1 和 0 分别表示。\n",
    "\n",
    "#### 开发流程\n",
    "\n",
    "```\n",
    "收集数据: 可以使用任何方法\n",
    "准备数据: 从文本中构建词向量\n",
    "分析数据: 检查词条确保解析的正确性\n",
    "训练算法: 从词向量计算概率\n",
    "测试算法: 根据现实情况修改分类器\n",
    "使用算法: 对社区留言板言论进行分类\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重点：理解贝叶斯条件概率公式\n",
    "\n",
    "`数学之美番外篇：平凡而又神奇的贝叶斯方法` 例子：穿长裤的人中女生的个数 = 既是女生还穿长裤的人数 ／ 所有穿长裤的人数【仅从概率方面来理解有点绕弯，直接用人数来做对比】\n",
    "\n",
    "$$ \n",
    "P(Girls | Pants) * total \\ people = \\frac {P(Girls \\ and \\ Pants) * total \\ people}{P(Pants) * total \\ people} \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "P(Girls | Pants) = \\frac {P(Girls \\ and \\ Pants)}{P(Pants)} \n",
    "$$\n",
    "\n",
    "`再来一个例子：`\n",
    "\n",
    "$$\n",
    "P(喜欢你的人中长得漂亮的人概率) = \\frac {P(既漂亮又喜欢你的人)} {P(喜欢你的人)}\n",
    "$$\n",
    "\n",
    "`一般化处理后：`\n",
    "\n",
    "$$\n",
    "P(A | B) = \\frac {P(AB)} {P(B)}      \n",
    "$$\n",
    "\n",
    "`然后根据概率论有：`\n",
    "\n",
    "$$\n",
    "P(AB) = P(A|B) * P(B) = P(B|A) * P(A)\n",
    "$$\n",
    "\n",
    "`所以可以更广泛的泛化为：`\n",
    "\n",
    "$$\n",
    "P(A | B) = \\frac {P(AB)} {P(B)} = \\frac {P(B|A) * P(A)} {P(B)} = \\frac {P(A|B) * P(B)} {P(B)}\n",
    "$$\n",
    "\n",
    "`所以，贝叶斯的核心是：通过条件概率，将 P(A|B) 和 P(B|A) 进行互相可推导化。其中A, B都可以进行扩展`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例1: 屏蔽社区留言板的侮辱性言论 | 朴素贝叶斯方法\n",
    "\n",
    "$$\n",
    "model : P(垃圾评论 | 单词) = \\frac {P(单词 | 垃圾评论) * P(垃圾评论)} {P(单词)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(垃圾评论 | 留言) = \\sum_{i}^{n} P(垃圾评论 | 单词_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    \"\"\"\n",
    "    创建数据集\n",
    "    :return: 单词列表postingList, 所属类别classVec\n",
    "    \"\"\"\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], #[0,0,1,1,1......]\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'gar e'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  # 1 is abusive, 0 not\n",
    "    return postingList, classVec\n",
    "\n",
    "postingList, classVec = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'love', 'help', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my', 'gar e']\n"
     ]
    }
   ],
   "source": [
    "def createVocabList(dataSet):\n",
    "    \"\"\"\n",
    "    获取所有单词的集合\n",
    "    :param dataSet: 数据集\n",
    "    :return: 所有单词的集合(即不含重复元素的单词列表)\n",
    "    \"\"\"\n",
    "    vocabSet = set([])  # create empty set\n",
    "    for document in dataSet:\n",
    "        # 操作符 | 用于求两个集合的并集\n",
    "        vocabSet = vocabSet | set(document)  # union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "vocabSet = createVocabList(postingList)\n",
    "print vocabSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    \"\"\"\n",
    "    遍历查看该单词是否出现，出现该单词则将该单词置1\n",
    "    :param vocabList: 所有单词集合列表\n",
    "    :param inputSet: 输入数据集\n",
    "    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中\n",
    "    \"\"\"\n",
    "    # 创建一个和词汇表等长的向量，并将其元素都设置为0\n",
    "    returnVec = [0] * len(vocabList)# [0,0......]\n",
    "    # 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _trainNB0(trainMatrix, trainCategory):\n",
    "    \"\"\"\n",
    "    训练数据原版\n",
    "    :param trainMatrix: 文件单词矩阵 [[1,0,1,1,1....],[],[]...]\n",
    "    :param trainCategory: 文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 侮辱性文件的出现概率，即trainCategory中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    \n",
    "    # 构造单词出现次数列表\n",
    "    p0Num = zeros(numWords) # [0,0,0,.....]\n",
    "    p1Num = zeros(numWords) # [0,0,0,.....]\n",
    "\n",
    "    # 整个数据集单词出现总数\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i] #[0,1,1,....]->[0,1,1,...]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        # 如果不是侮辱性文件，则计算非侮辱性文件中出现的侮辱性单词的个数\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "            \n",
    "    # 类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表\n",
    "    # 即 在1类别下，每个单词出现次数的占比\n",
    "    p1Vect = p1Num / p1Denom # [1,2,3,5]/90->[1/90,...]\n",
    "    \n",
    "    # 类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表\n",
    "    # 即 在0类别下，每个单词出现次数的占比\n",
    "    p0Vect = p0Num / p0Denom\n",
    "    \n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    \"\"\"\n",
    "    使用算法：\n",
    "        # 将乘法转换为加法\n",
    "        乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n",
    "        加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    :param vec2Classify: 待测数据[0,1,1,1,1...]，即要分类的向量\n",
    "    :param p0Vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    :param p1Vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    :param pClass1: 类别1，侮辱性文件的出现概率\n",
    "    :return: 类别1 or 0\n",
    "    \"\"\"\n",
    "    # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    # 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。\n",
    "    # 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来\n",
    "    # 可以理解为 1.单词在词汇表中的条件下，文件是good 类别的概率 也可以理解为 2.在整个空间下，文件既在词汇表中又是good类别的概率\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec\n",
    "\n",
    "\n",
    "def testingNB():\n",
    "    \"\"\"\n",
    "    测试朴素贝叶斯算法\n",
    "    \"\"\"\n",
    "    # 1. 加载数据集\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    \n",
    "    # 2. 创建单词集合，即所有信息中的关键字\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    \n",
    "    # 3. 计算单词是否出现并创建数据矩阵，这个矩阵估计会比较大哦，数据量大了的话需要定点优化了\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        # 返回m*len(myVocabList)的矩阵， 记录的都是0，1信息\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    \n",
    "    # 4. 训练数据，p0V: 在正常评论中每个单词出现的概率；p1V: 在垃圾评论中每个单词出现的概率；pAb: 垃圾评论的概率\n",
    "    p0V, p1V, pAb = _trainNB0(array(trainMat), array(listClasses))\n",
    "    \n",
    "    # 5. 测试数据\n",
    "    testEntry = ['love', 'my', 'dalmation', 'datayes']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb)\n",
    "    testEntry = ['stupid', 'garbage', 'datayes']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word: datayes is not in my Vocabulary!\n",
      "['love', 'my', 'dalmation', 'datayes'] classified as:  0\n",
      "the word: garbage is not in my Vocabulary!\n",
      "the word: datayes is not in my Vocabulary!\n",
      "['stupid', 'garbage', 'datayes'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 案例2: 使用朴素贝叶斯过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    '''\n",
    "    Desc:\n",
    "        接收一个大字符串并将其解析为字符串列表\n",
    "    Args:\n",
    "        bigString -- 大字符串\n",
    "    Returns:\n",
    "        去掉少于 2 个字符的字符串，并将所有字符串转换为小写，返回字符串列表\n",
    "    '''\n",
    "    import re\n",
    "    # 使用正则表达式来切分句子，其中分隔符是除单词、数字外的任意字符串\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['peter', 'with', 'jose', 'out', 'town', 'you', 'want', 'meet', 'once', 'while', 'keep', 'things', 'going', 'and', 'some', 'interesting', 'stuff', 'let', 'know', 'eugene']\n"
     ]
    }
   ],
   "source": [
    "def testParseTest():\n",
    "    print textParse(open('../../../input/4.NaiveBayes/email/ham/1.txt').read())\n",
    "\n",
    "testParseTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    \"\"\"\n",
    "    训练数据优化版本\n",
    "    :param trainMatrix: 文件单词矩阵\n",
    "    :param trainCategory: 文件对应的类别\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 侮辱性文件的出现概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "\n",
    "    # 构造单词出现次数列表\n",
    "    # p0Num 正常的统计\n",
    "    # p1Num 侮辱的统计 \n",
    "    # 避免单词列表中的任何一个单词为0，而导致最后的乘积为0，所以将每个单词的出现次数初始化为 1\n",
    "    p0Num = ones(numWords)#[0,0......]->[1,1,1,1,1.....]\n",
    "    p1Num = ones(numWords)\n",
    "\n",
    "    # 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）\n",
    "    # p0Denom 正常的统计\n",
    "    # p1Denom 侮辱的统计\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            # 累加辱骂词的频次\n",
    "            p1Num += trainMatrix[i]\n",
    "            # 对每篇文章的辱骂的频次 进行统计汇总\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    p1Vect = log(p1Num / p1Denom)\n",
    "    # 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    p0Vect = log(p0Num / p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the errorCount is:  5\n",
      "the testSet length is : 20\n",
      "the error rate is : 0.25\n"
     ]
    }
   ],
   "source": [
    "def spamTest():\n",
    "    '''\n",
    "    Desc:\n",
    "        对贝叶斯垃圾邮件分类器进行自动化处理。\n",
    "    Args:\n",
    "        none\n",
    "    Returns:\n",
    "        对测试集中的每封邮件进行分类，若邮件分类错误，则错误数加 1，最后返回总的错误百分比。\n",
    "    '''\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    \n",
    "    for i in range(1, 26):\n",
    "        # 切分，解析数据，并归类为 1 类别\n",
    "        wordList = textParse(open('../../../input/4.NaiveBayes/email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        classList.append(1)\n",
    "        # 切分，解析数据，并归类为 0 类别\n",
    "        wordList = textParse(open('../../../input/4.NaiveBayes/email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    \n",
    "    # 创建词汇表    \n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = range(50)\n",
    "    testSet = []\n",
    "    \n",
    "    # 随机取 10 个邮件用来测试\n",
    "    for i in range(20):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    \n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    \n",
    "    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))\n",
    "    \n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    \n",
    "    print 'the errorCount is: ', errorCount\n",
    "    print 'the testSet length is :', len(testSet)\n",
    "    print 'the error rate is :', float(errorCount)/len(testSet)\n",
    "\n",
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "\n",
    "- [数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)\n",
    "- [阿里云天池](https://tianchi.aliyun.com/)\n",
    "- [朴素贝叶斯理论推导与三种常见模型](http://blog.csdn.net/u012162613/article/details/48323777)\n",
    "- []()\n",
    "- []()\n",
    "- []()\n",
    "- []()\n",
    "- []()\n",
    "- []()\n",
    "- []()\n",
    "- []()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
